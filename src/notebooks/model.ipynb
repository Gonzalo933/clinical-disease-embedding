{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f8f5b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-07 16:01:09.139676: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-07 16:01:09.238474: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-11-07 16:01:09.238492: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-11-07 16:01:09.253981: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-07 16:01:09.695594: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-07 16:01:09.695640: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-07 16:01:09.695645: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# All source code and ideas are from: https://towardsdatascience.com/neural-network-embeddings-explained-4d028e6f0526\n",
    "# with some modifications\n",
    "\n",
    "\"\"\"\n",
    "Note to me: Another formulation of the problem is to fix the number of links but allow a \n",
    "variable number of articles (right now both are fixes)\n",
    "\n",
    "The input of the neural network would be a one hot encoded vector\n",
    "with each position (dim.) representing wether a link exists.\n",
    "\n",
    "Each vector represents an article.\n",
    "\"\"\"\n",
    "\n",
    "%matplotlib inline\n",
    "import functools\n",
    "import json\n",
    "from collections import Counter\n",
    "from glob import glob\n",
    "from itertools import chain\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from keras.layers import Dense, Dot, Embedding, Input, Reshape\n",
    "from keras.models import Model\n",
    "import numbers\n",
    "from tensorboard.plugins import projector\n",
    "\n",
    "\n",
    "articles_path = Path(\n",
    "    \"../../data/ndjson\"\n",
    ")  # \"/home/gonzalo/repos/wiki_disease/data/wiki/old/\"\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "plt.rcParams[\"font.size\"] = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce7cb23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Article:\n",
    "    def __init__(\n",
    "        self, title, infobox, wikilinks, external_links, timestamp, text_length\n",
    "    ):\n",
    "        self.title = title\n",
    "        self.infobox = infobox\n",
    "        self.wikilinks = wikilinks\n",
    "        self.external_links = external_links\n",
    "        self.timestamp = timestamp\n",
    "        self.text_length = text_length\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return (\n",
    "            self.title,\n",
    "            self.infobox,\n",
    "            self.wikilinks,\n",
    "            self.external_links,\n",
    "            self.timestamp,\n",
    "            self.text_length,\n",
    "        )[item]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"<Article: {self.title}>\"\n",
    "\n",
    "    def __format__(self, spec):\n",
    "        return self.title.__format__(spec)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.title)\n",
    "\n",
    "class ArticleCollection:\n",
    "    def __init__(self):\n",
    "        self.articles = []\n",
    "        # To map titles to index positions\n",
    "        self.article_index = {}\n",
    "        # To map index positions to titles\n",
    "        self.index_article = {}\n",
    "        self.embedding_weights = None\n",
    "\n",
    "    def append(self, *article_data):\n",
    "        self.articles.append(Article(*article_data))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.articles)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        if isinstance(item, numbers.Number):\n",
    "            return self.articles[int(item)]\n",
    "        elif isinstance(item, str):\n",
    "            return self._title_mapping()[item]\n",
    "        else:\n",
    "            raise IndexError(\"Invalid index\")\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.articles)\n",
    "\n",
    "    def _title_mapping(self):\n",
    "        # Create a mapping (article_index):   article_title -> article_index\n",
    "        if len(self.article_index) != len(self.articles):\n",
    "            self.article_index = {\n",
    "                article.title: idx for idx, article in enumerate(self.articles)\n",
    "            }\n",
    "        return self.article_index\n",
    "\n",
    "    def keys(self):\n",
    "        return self._title_mapping().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ded3598a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total articles: 8736\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Arteriovenous malformation'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load articles from json files\n",
    "articles = ArticleCollection()\n",
    "\n",
    "for f in articles_path.glob(\"*.ndjson\"):\n",
    "    with open(f, \"r\") as json_file:\n",
    "        for lines in json_file.readlines():\n",
    "            articles.append(*json.loads(lines))\n",
    "print(f\"Total articles: {len(articles)}\")\n",
    "articles[55].title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0dd9e777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "313"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles[\"Hume fracture\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ca2afe",
   "metadata": {},
   "source": [
    "The hypothesis for the project is that articles that are similar will point to similar (or the same) wikipedia links.\n",
    "\n",
    "Let's run some stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9ad351e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, OrderedDict\n",
    "\n",
    "\n",
    "def count_items(l):\n",
    "    \"\"\"Return ordered dictionary of counts of objects in `l`\"\"\"\n",
    "\n",
    "    # Create a counter object\n",
    "    counts = Counter(l)\n",
    "\n",
    "    # Sort by highest count first and place in ordered dictionary\n",
    "    counts = sorted(counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    counts = OrderedDict(counts)\n",
    "\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "681bdc52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('List of cutaneous conditions', 916),\n",
       " ('Category:Rare diseases', 668),\n",
       " ('Category:Wikipedia medicine articles ready to translate', 614),\n",
       " ('inflammation', 580),\n",
       " ('CT scan', 492),\n",
       " ('Category:Syndromes', 466),\n",
       " ('Micrograph', 447),\n",
       " ('infection', 440),\n",
       " ('World Health Organization', 440),\n",
       " ('fever', 440)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find set of wikilinks for each article\n",
    "unique_wikilinks = list(chain(*[list(set(article.wikilinks)) for article in articles]))\n",
    "\n",
    "wikilink_counts = count_items(unique_wikilinks)\n",
    "list(wikilink_counts.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9dddc066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 105388 unique wikilinks.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('list of cutaneous conditions', 916),\n",
       " ('inflammation', 680),\n",
       " ('category:rare diseases', 668),\n",
       " ('magnetic resonance imaging', 635),\n",
       " ('category:wikipedia medicine articles ready to translate', 614),\n",
       " ('fever', 551),\n",
       " ('infection', 527),\n",
       " ('ct scan', 508),\n",
       " ('micrograph', 508),\n",
       " ('cancer', 496)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikilinks = [link.lower() for link in unique_wikilinks]\n",
    "print(f\"There are {len(set(wikilinks))} unique wikilinks.\")\n",
    "\n",
    "wikilink_counts = count_items(wikilinks)\n",
    "list(wikilink_counts.items())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd654a90",
   "metadata": {},
   "source": [
    "We see that the most pointed wikilinks are usually categories within diseases. That's good and kind of helps sustaining the hypothesis.\n",
    "\n",
    "Some medical conditions (articles) might point to other medical conditions.\n",
    "Let's run the same ranking again but only using links for medical conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7e7574e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Depression (mood)', 187),\n",
       " (\"Parkinson's disease\", 175),\n",
       " ('Hypoxia (medical)', 142),\n",
       " ('HIV/AIDS', 135),\n",
       " ('Staphylococcus aureus', 134),\n",
       " (\"Crohn's disease\", 133),\n",
       " (\"Alzheimer's disease\", 125),\n",
       " ('Syncope (medicine)', 123),\n",
       " ('Down syndrome', 114),\n",
       " ('Fever', 111)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find set of wikilinks for each article\n",
    "unique_wikilinks_articles = list(\n",
    "    chain(\n",
    "        *[\n",
    "            list(set(link for link in article[2] if link in articles.keys()))\n",
    "            for article in articles\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "# Count the number of articles linked to by other articles\n",
    "wikilink_article_counts = count_items(unique_wikilinks_articles)\n",
    "list(wikilink_article_counts.items())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950d19cc",
   "metadata": {},
   "source": [
    "Let's build the same mappings for links. We aren't really interested in links used by few articles so we will use only links pointed by at least 5 articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0191db2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total links 15625\n"
     ]
    }
   ],
   "source": [
    "MIN_ARTICLES_POINTING = 5\n",
    "links = [t[0] for t in wikilink_counts.items() if t[1] >= MIN_ARTICLES_POINTING]\n",
    "link_index = {link: idx for idx, link in enumerate(links)}\n",
    "index_link = {idx: link for link, idx in link_index.items()}\n",
    "\n",
    "link_index[\"neurology\"]\n",
    "index_link[300]\n",
    "print(f\"Total links {len(links)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e439bc",
   "metadata": {},
   "source": [
    "## Create dataset\n",
    "\n",
    "We have each of the articles and links encoded as integers and we can now proceed to the next step.\n",
    "\n",
    "We want to create a NN that given the tuple (article, link) tries to predict wether the link exists and returns ($y$) 0 if it doesn't and 1 if it does.\n",
    "\n",
    "Let's do the easy part first. Let's obtain all the \"positive\" tuple. That is, all the connections between articles and links that exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3538d1e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(375532, 15625, 8736)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs = []\n",
    "\n",
    "for article in articles:\n",
    "    # Iterate through the links in the article and add them in batch (.extend)\n",
    "    pairs.extend(\n",
    "        (articles[article.title], link_index[link.lower()])\n",
    "        for link in article.wikilinks\n",
    "        if link.lower() in links\n",
    "    )\n",
    "\n",
    "pairs_set = set(pairs)\n",
    "\n",
    "len(pairs), len(links), len(articles)\n",
    "# pairs[5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "631c1fd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<Article: Recurrent painful ophthalmoplegic neuropathy>,\n",
       " 'digital subtraction angiography')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles[pairs[50][0]], index_link[pairs[50][1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfddadb",
   "metadata": {},
   "source": [
    "Creating negative examples is also easy. We just take a random article and a random link and if the tuple doesn't exist in \"pairs\" that means that the connection does not exist ($y=0$).\n",
    "\n",
    "We can implement everything in a generator function to train the NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb4265fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "random.seed(100)\n",
    "\n",
    "\n",
    "def generate_batch(pairs, n_positive=50, negative_ratio=1.0, classification=False):\n",
    "    \"\"\"Generate batches of samples for training\"\"\"\n",
    "    batch_size = n_positive * (1 + negative_ratio)\n",
    "    batch = np.zeros((batch_size, 3))\n",
    "\n",
    "    # Adjust label based on task\n",
    "    if classification:\n",
    "        neg_label = 0\n",
    "    else:\n",
    "        neg_label = -1\n",
    "\n",
    "    # This creates a generator\n",
    "    while True:\n",
    "        # randomly choose positive examples\n",
    "        for idx, (article_id, link_id) in enumerate(random.sample(pairs, n_positive)):\n",
    "            batch[idx, :] = (article_id, link_id, 1)\n",
    "\n",
    "        # Add negative examples until reach batch size\n",
    "        while idx < batch_size:\n",
    "\n",
    "            # random selection\n",
    "            random_article = random.randrange(len(articles))\n",
    "            random_link = random.randrange(len(links))\n",
    "\n",
    "            # Check to make sure this is not a positive example\n",
    "            if (random_article, random_link) not in pairs_set:\n",
    "\n",
    "                # Add to batch and increment index\n",
    "                batch[idx, :] = (random_article, random_link, neg_label)\n",
    "                idx += 1\n",
    "\n",
    "        # Make sure to shuffle order\n",
    "        np.random.shuffle(batch)\n",
    "        yield {\"article\": batch[:, 0], \"link\": batch[:, 1]}, batch[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "514720b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'article': array([7453., 8304., 1519., 5730., 6439., 2863.]),\n",
       "  'link': array([15592., 13107.,  2504.,  7102., 11992., 11559.])},\n",
       " array([-1., -1.,  1., -1., -1., -1.]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We ask for 2 positive samples with a negative ratio of 2 for a total of 2 positive 4 negative.\n",
    "next(generate_batch(pairs, n_positive=2, negative_ratio=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8d34802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article: Perisylvian syndrome           Link: category:congenital disorders of nervous system Label: -1.0\n",
      "Article: Epidemic dropsy                Link: mitochondrion                            Label: 1.0\n",
      "Article: Ring chromosome 14 syndrome    Link: interleukin 2                            Label: -1.0\n",
      "Article: Glycogen storage disease type IX Link: haemophilus                              Label: -1.0\n",
      "Article: Lichen spinulosus              Link: retinoic acid                            Label: -1.0\n",
      "Article: Basal-cell carcinoma           Link: ventilation (physiology)                 Label: -1.0\n"
     ]
    }
   ],
   "source": [
    "x, y = next(generate_batch(pairs, n_positive=2, negative_ratio=2))\n",
    "\n",
    "\n",
    "for label, b_idx, l_idx in zip(y, x[\"article\"], x[\"link\"]):\n",
    "    print(f\"Article: {articles[b_idx]:30} Link: {index_link[l_idx]:40} Label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf965e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " article (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " link (InputLayer)              [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " article_embedding (Embedding)  (None, 1, 50)        436800      ['article[0][0]']                \n",
      "                                                                                                  \n",
      " link_embedding (Embedding)     (None, 1, 50)        781250      ['link[0][0]']                   \n",
      "                                                                                                  \n",
      " dot_product (Dot)              (None, 1, 1)         0           ['article_embedding[0][0]',      \n",
      "                                                                  'link_embedding[0][0]']         \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 1)            0           ['dot_product[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,218,050\n",
      "Trainable params: 1,218,050\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-07 16:01:47.047402: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-11-07 16:01:47.047426: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-11-07 16:01:47.047442: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (gon-laptop-linux): /proc/driver/nvidia/version does not exist\n",
      "2022-11-07 16:01:47.047585: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "def article_embedding_model(embedding_size=50, classification=False):\n",
    "    \"\"\"Model to embed articles and wikilinks.\n",
    "    Trained to discern if a link is present in a article.\n",
    "    Embedding layers will try to 'put' similar articles and links close in the 50-dimensional space.\n",
    "    \"\"\"\n",
    "\n",
    "    # Both inputs are 1-dimensional\n",
    "    article = Input(name=\"article\", shape=[1])\n",
    "    link = Input(name=\"link\", shape=[1])\n",
    "\n",
    "    # Embedding the article (shape will be (None, 1, 50))\n",
    "    article_embedding = Embedding(\n",
    "        name=\"article_embedding\",\n",
    "        input_dim=len(articles),\n",
    "        output_dim=embedding_size,\n",
    "    )(article)\n",
    "\n",
    "    # Embedding the link (shape will be (None, 1, 50))\n",
    "    link_embedding = Embedding(\n",
    "        name=\"link_embedding\", input_dim=len(link_index), output_dim=embedding_size\n",
    "    )(link)\n",
    "\n",
    "    # Merge the layers with a dot product along the second axis (shape will be (None, 1, 1))\n",
    "    merged = Dot(name=\"dot_product\", normalize=True, axes=2)(\n",
    "        [article_embedding, link_embedding]\n",
    "    )\n",
    "\n",
    "    # Reshape to be a single number (shape will be (None, 1))\n",
    "    merged = Reshape(target_shape=[1])(merged)\n",
    "\n",
    "    # If classifcation, add extra layer and loss function is binary cross entropy\n",
    "    if classification:\n",
    "        merged = Dense(1, activation=\"sigmoid\")(merged)\n",
    "        model = Model(inputs=[book, link], outputs=merged)\n",
    "        model.compile(\n",
    "            optimizer=\"Adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n",
    "        )\n",
    "\n",
    "    # Otherwise loss function is mean squared error\n",
    "    else:\n",
    "        model = Model(inputs=[article, link], outputs=merged)\n",
    "        model.compile(optimizer=\"Adam\", loss=\"mse\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Instantiate model and show parameters\n",
    "model = article_embedding_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0b626b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_49584/3019650480.py:6: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  h = model.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "366/366 - 4s - loss: 0.9726 - 4s/epoch - 12ms/step\n",
      "Epoch 2/15\n",
      "366/366 - 4s - loss: 0.7664 - 4s/epoch - 12ms/step\n",
      "Epoch 3/15\n",
      "366/366 - 4s - loss: 0.5182 - 4s/epoch - 12ms/step\n",
      "Epoch 4/15\n",
      "366/366 - 4s - loss: 0.4703 - 4s/epoch - 11ms/step\n",
      "Epoch 5/15\n",
      "366/366 - 4s - loss: 0.4501 - 4s/epoch - 11ms/step\n",
      "Epoch 6/15\n",
      "366/366 - 4s - loss: 0.4410 - 4s/epoch - 12ms/step\n",
      "Epoch 7/15\n",
      "366/366 - 5s - loss: 0.4371 - 5s/epoch - 13ms/step\n",
      "Epoch 8/15\n",
      "366/366 - 5s - loss: 0.4339 - 5s/epoch - 13ms/step\n",
      "Epoch 9/15\n",
      "366/366 - 5s - loss: 0.4327 - 5s/epoch - 13ms/step\n",
      "Epoch 10/15\n",
      "366/366 - 4s - loss: 0.4310 - 4s/epoch - 12ms/step\n",
      "Epoch 11/15\n"
     ]
    }
   ],
   "source": [
    "n_positive = 1024\n",
    "\n",
    "gen = generate_batch(pairs, n_positive, negative_ratio=2)\n",
    "\n",
    "# Train\n",
    "h = model.fit_generator(\n",
    "    gen, epochs=15, steps_per_epoch=len(pairs) // n_positive, verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ade42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_weights(name, model):\n",
    "    \"\"\"Extract weights from a neural network model\"\"\"\n",
    "\n",
    "    # Extract weights\n",
    "    weight_layer = model.get_layer(name)\n",
    "    weights = weight_layer.get_weights()[0]\n",
    "\n",
    "    # Normalize\n",
    "    weights = weights / np.linalg.norm(weights, axis=1).reshape((-1, 1))\n",
    "    return weights\n",
    "\n",
    "\n",
    "article_weights = extract_weights(\"article_embedding\", model)\n",
    "link_weights = extract_weights(\"link_embedding\", model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f85a87",
   "metadata": {},
   "source": [
    "We want to calculate similarity (dot product) between pairs of article embeddings because we expected the neural network to have placed similar articles closer together in the 50-dimensional space.\n",
    "\n",
    "Let's run a simple test, let's check how similar is an article with itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c062d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Similarity of article with index 0 with itself: {np.dot(article_weights[0], article_weights[0])}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478cdada",
   "metadata": {},
   "source": [
    "Hmm, that does not make a lot of sense, an article should be similar with itsel. In fact the similarity should be 1.\n",
    "\n",
    "That's okay, we just need to normalize our embeddings:\n",
    "\n",
    "$$ \\lbrace \\frac{W_n}{\\lVert W \\rVert}_{n} {\\rbrace}_{n=0}^{N}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b40432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.allclose(np.linalg.norm(article_weights, axis = 1), np.sqrt(np.sum(article_weights**2, axis=1)))\n",
    "article_weights = article_weights / np.linalg.norm(article_weights, axis=1).reshape(\n",
    "    (-1, 1)\n",
    ")\n",
    "np.sum(np.square(article_weights[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48b0d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "link_weights = link_weights / np.linalg.norm(link_weights, axis=1).reshape((-1, 1))\n",
    "np.sum(np.square(link_weights[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2a030b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Similarity of article with index 0 with itself (after normalization): {np.dot(article_weights[0], article_weights[0])}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8261e57d",
   "metadata": {},
   "source": [
    "That's way better. We can now implement a function to find similar articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db93f081",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_embeddings_path = Path(\"../../data/embeddings\")\n",
    "output_embeddings_path.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7ce652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save embeddings\n",
    "np.savetxt(\n",
    "    output_embeddings_path / Path(\"article_embedding.tsv\"),\n",
    "    article_weights,\n",
    "    delimiter=\"\\t\",\n",
    ")\n",
    "np.savetxt(\n",
    "    output_embeddings_path / Path(\"link_embedding.tsv\"), link_weights, delimiter=\"\\t\"\n",
    ")\n",
    "\n",
    "# Save metadata\n",
    "\n",
    "with open(output_embeddings_path / Path(\"link_metadata.tsv\"), \"w\", encoding=\"utf-8\") as fout:\n",
    "    for l in link_index.keys():\n",
    "        fout.write(str(l))\n",
    "        fout.write(\"\\n\")\n",
    "        \n",
    "with open(output_embeddings_path / Path(\"article_metadata.tsv\"), \"w\", encoding=\"utf-8\") as fout:\n",
    "    for l in articles.keys():\n",
    "        fout.write(str(l))\n",
    "        fout.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de7f2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar(\n",
    "    name,\n",
    "    weights,\n",
    "    index_name=\"article\",\n",
    "    n=15,\n",
    "    least=False,\n",
    "    return_dist=False,\n",
    "    plot=False,\n",
    "):\n",
    "    \"\"\"Find n most similar items (or least) to name based on embeddings. Option to also plot the results\"\"\"\n",
    "\n",
    "    # Select index and reverse index\n",
    "    if index_name == \"article\":\n",
    "        index = articles\n",
    "        rindex = articles\n",
    "    elif index_name == \"link\":\n",
    "        index = link_index\n",
    "        rindex = index_link\n",
    "\n",
    "    # Check to make sure `name` is in index\n",
    "    try:\n",
    "        # Calculate dot product between book and all others\n",
    "        dists = np.dot(weights, weights[index[name]])\n",
    "    except KeyError:\n",
    "        print(f\"{name} Not Found.\")\n",
    "        return\n",
    "\n",
    "    # Sort distance indexes from smallest to largest\n",
    "    sorted_dists = np.argsort(dists)\n",
    "\n",
    "    # Plot results if specified\n",
    "    if plot:\n",
    "\n",
    "        # Find furthest and closest items\n",
    "        furthest = sorted_dists[: (n // 2)]\n",
    "        closest = sorted_dists[-n - 1 : len(dists) - 1]\n",
    "        items = [rindex[c] for c in furthest]\n",
    "        items.extend(rindex[c] for c in closest)\n",
    "\n",
    "        # Find furthest and closets distances\n",
    "        distances = [dists[c] for c in furthest]\n",
    "        distances.extend(dists[c] for c in closest)\n",
    "\n",
    "        colors = [\"r\" for _ in range(n // 2)]\n",
    "        colors.extend(\"g\" for _ in range(n))\n",
    "\n",
    "        data = pd.DataFrame({\"distance\": distances}, index=items)\n",
    "\n",
    "        # Horizontal bar chart\n",
    "        data[\"distance\"].plot.barh(\n",
    "            color=colors, figsize=(10, 8), edgecolor=\"k\", linewidth=2\n",
    "        )\n",
    "        plt.xlabel(\"Cosine Similarity\")\n",
    "        plt.axvline(x=0, color=\"k\")\n",
    "\n",
    "        # Formatting for italicized title\n",
    "        name_str = f\"{index_name.capitalize()}s Most and Least Similar to\"\n",
    "        for word in name.split():\n",
    "            # Title uses latex for italize\n",
    "            name_str += \" $\\it{\" + word + \"}$\"\n",
    "        plt.title(name_str, x=0.2, size=28, y=1.05)\n",
    "\n",
    "        return None\n",
    "\n",
    "    # If specified, find the least similar\n",
    "    if least:\n",
    "        # Take the first n from sorted distances\n",
    "        closest = sorted_dists[:n]\n",
    "\n",
    "        print(f\"{index_name.capitalize()}s furthest from {name}.\\n\")\n",
    "\n",
    "    # Otherwise find the most similar\n",
    "    else:\n",
    "        # Take the last n sorted distances\n",
    "        closest = sorted_dists[-n:]\n",
    "\n",
    "        # Need distances later on\n",
    "        if return_dist:\n",
    "            return dists, closest\n",
    "\n",
    "        print(f\"{index_name.capitalize()}s closest to {name}.\\n\")\n",
    "\n",
    "    # Need distances later on\n",
    "    if return_dist:\n",
    "        return dists, closest\n",
    "\n",
    "    # Print formatting\n",
    "    max_width = max([len(rindex[c]) for c in closest])\n",
    "\n",
    "    # Print the most similar and distances\n",
    "    for c in reversed(closest):\n",
    "        print(\n",
    "            f\"{index_name.capitalize()}: {rindex[c]:{max_width + 2}} Similarity: {dists[c]:.{2}}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c994dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_similar(\"Fever\", article_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8850f78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_similar(\"Sepsis\", article_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43ac67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_similar(\"Parkinson's disease\", article_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd4a455",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_similar(\"Sleep apnea\", article_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf34e4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = \"\"\"embeddings {\n",
    "  metadata_path: \"article_metadata.tsv\"\n",
    "  tensor_path: \"article_embedding.tsv\"\n",
    "}\n",
    "\"\"\"\n",
    "with open(output_embeddings_path / Path(\"projector_config.pbtxt\"), \"w\") as f:\n",
    "    f.write(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087f506b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_dir = str(output_embeddings_path.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6a79e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For some reason I have to run this cell twice because of an error: \n",
    "# No dashboards are active for the current data set.\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir \"$tensorboard_dir\" --port=6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b368dec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Can we get a disease from symptons?\n",
    "\n",
    "custom_weight = (\n",
    "    link_weights[link_index[\"fever\"]]\n",
    "    + link_weights[link_index[\"headache\"]]\n",
    "    + link_weights[link_index[\"fatigue\"]]\n",
    ")\n",
    "dists = np.dot(link_weights, custom_weight)\n",
    "sorted_dists = np.argsort(dists)\n",
    "closest = sorted_dists[-15 - 1 : len(dists) - 1]\n",
    "[articles[c].title for c in closest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edff57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_weight = (\n",
    "    article_weights[articles[\"Fever\"]]\n",
    "    + article_weights[articles[\"Headache\"]]\n",
    "    + article_weights[articles[\"Fatigue\"]]\n",
    ")\n",
    "dists = np.dot(article_weights, custom_weight)\n",
    "sorted_dists = np.argsort(dists)\n",
    "closest = sorted_dists[-15 - 1 : len(dists) - 1]\n",
    "[articles[c].title for c in closest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00071805",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_names = [article.title for article in articles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a8acaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(article_names, columns=[\"title\"])\n",
    "df[df.title.str.contains(\"Fever\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291493dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
